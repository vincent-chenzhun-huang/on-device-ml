{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:03.364869Z",
     "start_time": "2023-10-29T00:02:03.340161Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bb2f3b410>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655f662e789486e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:03.590616Z",
     "start_time": "2023-10-29T00:02:03.572686Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_config = {\n",
    "    'input_size': 400,\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'hidden_layers': 2,\n",
    "    'hidden_size': 1024,\n",
    "    'epochs': 2,\n",
    "    'log_interval': 10, # log every 10 batches\n",
    "    'output_size': 10\n",
    "}\n",
    "\n",
    "# mnist_config = {\n",
    "#     'input_size': 784,\n",
    "#     'batch_size': 64,\n",
    "#     'lr': 0.001,\n",
    "#     'hidden_layers': 2,\n",
    "#     'hidden_size': 512,\n",
    "#     'epochs': 2,\n",
    "#     'log_interval': 10, # log every 10 batches\n",
    "#     'output_size': 10\n",
    "# }\n",
    "\n",
    "sst_config = {\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'hidden_layers': 3,\n",
    "    'hidden_size': 256,\n",
    "    'epochs': 2,\n",
    "    'log_interval': 10,\n",
    "    'output_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bec1910d5c27f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a65d0552d834f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:04.708777Z",
     "start_time": "2023-10-29T00:02:04.698508Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SSTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data_type='train', suffix='tsv', vocab=None, top_k=0, remove_stopwords=False):\n",
    "        self.datapath = os.path.join(data_dir, data_type + '.' + suffix)\n",
    "        self.data_type = data_type\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "\n",
    "        with open(self.datapath) as data_f:\n",
    "            for line in data_f:\n",
    "                sentence, label = line.strip().split('\\t')\n",
    "                self.sentences.append(sentence)\n",
    "                self.labels.append(label)\n",
    "        self.sentences = self.sentences[1:]\n",
    "        self.labels = self.labels[1:]\n",
    "        if vocab is None:\n",
    "            self.vocab = self.build_vocab()\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        if top_k > 0:\n",
    "            self.vocab = self.vocab[:top_k]\n",
    "        if remove_stopwords:\n",
    "            self.vocab = self.remove_stopwords()\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # add OOV token\n",
    "        if vocab is None:\n",
    "            self.vocab.append('<OOV>')\n",
    "            self.word2idx['<OOV>'] = len(self.vocab) - 1\n",
    "            self.idx2word[len(self.vocab) - 1] = '<OOV>'\n",
    "            \n",
    "    def remove_stopwords(self):\n",
    "        # remove stopwords obtained from nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        return [word for word in self.vocab if word not in stop_words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def one_hot_encode(self, word):\n",
    "        vector = torch.zeros(len(self.vocab))\n",
    "        if word in self.word2idx:\n",
    "            vector[self.word2idx[word]] += 1\n",
    "        else:\n",
    "            vector[self.word2idx['<OOV>']] += 1\n",
    "        return vector\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoded_sentence = torch.sum(torch.stack([self.one_hot_encode(word) for word in sentence.split()]), dim=0)\n",
    "        encoded_label = torch.tensor(int(label)) # 0: negative, 1: positive\n",
    "        return encoded_sentence, encoded_label\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab = {}\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence.split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "        return sorted(vocab, key=vocab.get, reverse=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632562d5f4c216c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71ca22088b15399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:05.834736Z",
     "start_time": "2023-10-29T00:02:05.815413Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data_type='train', suffix='csv', transforms=None):\n",
    "        self.datapath = os.path.join(data_dir, f'mnist_{data_type}.{suffix}')\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(self.datapath) as data_f:\n",
    "            line_cnt = 0\n",
    "            for line in data_f:\n",
    "                if line_cnt > 0:\n",
    "                    data = line.strip().split(',')\n",
    "                    self.labels.append(data[0])\n",
    "                    self.data.append(torch.tensor([float(dp) for dp in data[1:]]))\n",
    "                line_cnt += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx]\n",
    "        # normalize the data\n",
    "        data = (data - torch.mean(data)) / torch.std(data)\n",
    "        if self.transforms is not None:\n",
    "            # reconstruct to image and do transforms\n",
    "            data = data.reshape(1, 28, 28)\n",
    "            data = self.transforms(data)\n",
    "            # flatten\n",
    "            data = data.reshape(-1)\n",
    "        return data, torch.tensor(int(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc8e3714b50a46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b112e66d849660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:06.936730Z",
     "start_time": "2023-10-29T00:02:06.925034Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.total_flops = 0\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_idx in range(self.num_hidden_layers):\n",
    "            if layer_idx == 0:\n",
    "                self.layers.append(torch.nn.Linear(self.input_size, self.hidden_size))\n",
    "                self.total_flops += self.input_size * self.hidden_size\n",
    "            else:\n",
    "                self.layers.append(torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "                self.total_flops += self.hidden_size * self.hidden_size\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.total_flops += self.hidden_size\n",
    "        self.layers.append(torch.nn.Linear(self.hidden_size, self.output_size))\n",
    "        self.total_flops += self.hidden_size * self.output_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.total_flops += self.output_size * np.log2(self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_size)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.softmax(x) \n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe6efdb3772bc4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb9c4ad3dc03d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:00:11.045603Z",
     "start_time": "2023-10-29T01:00:11.039532Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 train_dl, \n",
    "                 valid_dl, \n",
    "                 config, \n",
    "                 ckpt_path='ckpt', \n",
    "                 device='cuda', \n",
    "                 save_ckpts=False,\n",
    "                 no_train=False):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.config = config\n",
    "        self.no_train = no_train\n",
    "        if not no_train:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['lr'])\n",
    "            self.criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.training_time = 0\n",
    "        self.training_time_per_epoch = []\n",
    "        self.total_inference_time = 0\n",
    "        self.inference_time = []\n",
    "        self.avg_inference_time_per_epoch = []\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.best_acc = 0\n",
    "        self.device = device\n",
    "        self.save_ckpt = save_ckpts\n",
    "\n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            start_time = time.time()\n",
    "            self.train_one_epoch()\n",
    "            end_time = time.time()\n",
    "            self.training_time_per_epoch.append(end_time - start_time)\n",
    "            acc = self.validate()\n",
    "            if acc > self.best_acc:\n",
    "                self.best_acc = acc\n",
    "                if self.save_ckpt:\n",
    "                    self.save(epoch)\n",
    "        end = time.time()\n",
    "        self.training_time = end - start\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        batch_bar = tqdm.tqdm(total=len(self.train_dl), dynamic_ncols=True, leave=True, position=0, desc='Train',\n",
    "                              ncols=5)\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        total_inference_epoch = 0\n",
    "\n",
    "        for idx, (train_data, train_label) in enumerate(self.train_dl):\n",
    "            unit_num_correct, unit_total_loss, batch_inference_time = self.train_one_batch(train_data, train_label)\n",
    "            total_inference_epoch += batch_inference_time\n",
    "            num_correct += unit_num_correct\n",
    "            total_loss += unit_total_loss\n",
    "            batch_bar.set_postfix(\n",
    "                acc=f\"{100 * num_correct / (self.config['batch_size'] * (idx + 1)):.4f}\",\n",
    "                loss=f\"{total_loss / (self.config['batch_size'] * (idx + 1)):.4f}\",\n",
    "                num_correct=f\"{num_correct}\",\n",
    "                lr=f\"{self.optimizer.param_groups[0]['lr']:.4f}\"\n",
    "            )\n",
    "            batch_bar.update()\n",
    "\n",
    "        self.avg_inference_time_per_epoch.append(total_inference_epoch / len(self.train_dl.dataset))\n",
    "        batch_bar.close()\n",
    "\n",
    "    def train_one_batch(self, train_data, train_label):\n",
    "        train_data = train_data.to(self.device)\n",
    "        train_label = train_label.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        inference_start = time.time()\n",
    "        output = self.model(train_data)\n",
    "        inference_end = time.time()\n",
    "        self.total_inference_time += inference_end - inference_start\n",
    "        # l2 regularization\n",
    "        # loss = self.criterion(output, train_label) - 0.001 * torch.norm(self.model.layers[0].weight, p=2)\n",
    "        loss = self.criterion(output, train_label)\n",
    "        num_correct = torch.sum(torch.argmax(output, dim=1) == train_label).item()\n",
    "        total_loss = float(loss.item())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return num_correct, total_loss, inference_end - inference_start\n",
    "\n",
    "    def validate(self):\n",
    "        # report accuracy\n",
    "        correct_count = 0\n",
    "        valid_loss = 0\n",
    "        num_samples = 0\n",
    "        for idx, (valid_data, valid_label) in enumerate(self.valid_dl):\n",
    "            num_samples += len(valid_data)\n",
    "            valid_data = valid_data.to(self.device)\n",
    "            valid_label = valid_label.to(self.device)\n",
    "            output = self.model(valid_data)\n",
    "            pred = torch.argmax(output, dim=1)  # (batch_size)\n",
    "            correct_count += torch.sum(pred == valid_label).item()\n",
    "            if not self.no_train:\n",
    "                loss = self.criterion(output, valid_label).item()\n",
    "                valid_loss += loss\n",
    "        if not self.no_train:\n",
    "            valid_loss /= num_samples\n",
    "        acc = correct_count / len(self.valid_dl.dataset)\n",
    "        print(f'\\nValidation Loss: {valid_loss if not self.no_train else \"n/a\"}\\t Validation Accuracy: {acc:.4f}')\n",
    "        self.valid_acc = acc\n",
    "        return acc\n",
    "    \n",
    "    def calculate_inference_latency(self, val_dl):\n",
    "        # calculate inference latency per batch\n",
    "        total_inference_time = 0\n",
    "        num_of_batches = 0\n",
    "        for idx, (valid_data, valid_label) in enumerate(val_dl):\n",
    "            num_of_batches += 1\n",
    "            valid_data = valid_data.to(self.device)\n",
    "            inference_start = time.time()\n",
    "            _ = self.model(valid_data)\n",
    "            inference_end = time.time()\n",
    "            total_inference_time += inference_end - inference_start\n",
    "        return total_inference_time / num_of_batches\n",
    "\n",
    "    def save(self, epoch):\n",
    "        ckpt_dir = os.path.join(self.ckpt_path, f'{time.strftime(\"%m-%d-%H-%M\", time.localtime())}')\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        print(f'Saving model at Epoch {epoch}')\n",
    "        ckpt_path = os.path.join(ckpt_dir, f'best_acc.pt')\n",
    "        torch.save(self.model.state_dict(), ckpt_path)\n",
    "\n",
    "    def get_training_stats(self):\n",
    "        # we need flops, avg inference per sample and accuracy\n",
    "        return {\n",
    "            'num_parameters': self.model.param_count,\n",
    "            'num_flops': self.model.flops,\n",
    "            'training_time': self.training_time,\n",
    "            'training_time_per_epoch': np.mean(self.training_time_per_epoch),\n",
    "            'best_acc': self.best_acc,\n",
    "            'average_inference_time_per_sample': self.total_inference_time / (self.config['epochs'] * len(self.train_dl.dataset)),\n",
    "        }\n",
    "    def report(self):\n",
    "        print(f'Number of parameters: {self.model.count_parameters()}')\n",
    "        print(f'Total number of FLOPs: {self.model.total_flops}')\n",
    "        print(f'Average training time per epoch: {np.mean(self.training_time_per_epoch)} seconds')\n",
    "        print(f'Training time: {self.training_time} seconds')\n",
    "        print(\n",
    "            f'Average inference time per sample: {self.total_inference_time / (self.config[\"epochs\"] * len(self.train_dl.dataset))} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e427d5891938274",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38dacbac82ea52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:03:58.899397Z",
     "start_time": "2023-10-29T00:03:50.310844Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "from torchvision import transforms\n",
    "\n",
    "sst_train_dataset = SSTDataset(data_dir='SST-2', data_type='train', top_k=5000)\n",
    "sst_dev_dataset = SSTDataset(data_dir='SST-2', data_type='dev', vocab=sst_train_dataset.vocab)\n",
    "mnist_train_dataset = MNISTDataset(data_dir='MNIST', data_type='train', transforms=transforms.CenterCrop((20, 20)))\n",
    "mnist_valid_dataset = MNISTDataset(data_dir='MNIST', data_type='test', transforms=transforms.CenterCrop((20, 20)))\n",
    "\n",
    "\n",
    "sst_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=sst_train_dataset,\n",
    "    batch_size=sst_config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "sst_dev_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=sst_dev_dataset,\n",
    "    batch_size=sst_config['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "mnist_dataloader = torch.utils.data.DataLoader(dataset=mnist_train_dataset,\n",
    "                                               batch_size=mnist_config['batch_size'],\n",
    "                                               shuffle=True)\n",
    "mnist_valid_dataloader = torch.utils.data.DataLoader(dataset=mnist_valid_dataset,\n",
    "                                                     batch_size=mnist_config['batch_size'],\n",
    "                                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8f6dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5001])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d813cdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46234eb11c0a5e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:03:59.853293Z",
     "start_time": "2023-10-29T00:03:59.835002Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce7a477a541fd203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:04:14.425035Z",
     "start_time": "2023-10-29T00:04:00.324277Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:14<00:00, 62.94it/s, acc=80.2322, loss=0.0259, lr=0.0010, num_correct=48165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.025196940413188047\t Validation Accuracy: 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:14<00:00, 65.64it/s, acc=87.5999, loss=0.0248, lr=0.0010, num_correct=52588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.023892757439329594\t Validation Accuracy: 0.9386\n",
      "Number of parameters: 1470474\n",
      "Total number of FLOPs: 1470497.2192809489\n",
      "Average training time per epoch: 14.599167823791504 seconds\n",
      "Training time: 30.736724138259888 seconds\n",
      "Average inference time per sample: 2.0052999688692118e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "# MNIST model\n",
    "mnist_model = MLP(input_size=mnist_config['input_size'], hidden_size=mnist_config['hidden_size'], num_hidden_layers=mnist_config['hidden_layers'], output_size=mnist_config['output_size'])\n",
    "\n",
    "mnist_trainer = Trainer(\n",
    "    model=mnist_model,\n",
    "    train_dl=mnist_dataloader,\n",
    "    valid_dl=mnist_valid_dataloader,\n",
    "    config=mnist_config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "mnist_trainer.train()\n",
    "mnist_trainer.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134b0ce4c68f679c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:06:31.509950Z",
     "start_time": "2023-10-29T00:06:09.970171Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1053/1053 [00:21<00:00, 49.84it/s, acc=82.3718, loss=0.0074, lr=0.0010, num_correct=55512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.007868469444983596\t Validation Accuracy: 0.8257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1053/1053 [00:21<00:00, 48.83it/s, acc=89.6160, loss=0.0064, lr=0.0010, num_correct=60394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.007981193721841235\t Validation Accuracy: 0.8119\n"
     ]
    }
   ],
   "source": [
    "# SST model\n",
    "sst_model = MLP(input_size=len(sst_train_dataset.vocab), \n",
    "                hidden_size=sst_config['hidden_size'], \n",
    "                num_hidden_layers=sst_config['hidden_layers'], \n",
    "                output_size=sst_config['output_size'])\n",
    "\n",
    "sst_trainer = Trainer(\n",
    "    model=sst_model,\n",
    "    train_dl=sst_dataloader,\n",
    "    valid_dl=sst_dev_dataloader,\n",
    "    config=sst_config,\n",
    "    device=device\n",
    ")\n",
    "sst_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd005a9a2306f8b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:28.890678Z",
     "start_time": "2023-10-29T00:42:28.876450Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get stats for baseline, \n",
    "#   - inference latency for batch size 1 and 64 over 5 runs\n",
    "#   - model size\n",
    "#   - parameter counts\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    # print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "def count_trainable_parameters(quantized_mode):\n",
    "    total_trainable_params = 0\n",
    "    for module in quantized_mode.modules():\n",
    "        if hasattr(module, '_packed_params'):\n",
    "            if isinstance(module, torch.nn.quantized.dynamic.Linear):\n",
    "                weight, bias = module._packed_params.unpack()\n",
    "                \n",
    "                if weight.requires_grad:\n",
    "                    total_trainable_params += weight.numel()\n",
    "                if bias is not None and bias.requires_grad:\n",
    "                    total_trainable_params += bias.numel()\n",
    "    return total_trainable_params\n",
    "\n",
    "def get_stats(num_runs,\n",
    "              model,\n",
    "              trainer,\n",
    "              val_ds,\n",
    "              model_type):\n",
    "    stats = {}\n",
    "    stats['model_name'] = model_type\n",
    "\n",
    "    stats['acc'] = trainer.valid_acc\n",
    "    if model_type.startswith('mnist'):\n",
    "        print(f'model name: {model_type}')\n",
    "        for batch_size in [64, 1]:\n",
    "            print(f'Running for batch size {batch_size}')\n",
    "            avg_inference_latency = []\n",
    "            for _ in range(num_runs):\n",
    "                val_dl = torch.utils.data.DataLoader(\n",
    "                    dataset=val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False\n",
    "                )\n",
    "                stats['model_size_mb'] = print_size_of_model(model) / 1e6\n",
    "                stats['parameter_count'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                avg_inference_latency.append(trainer.calculate_inference_latency(val_dl))\n",
    "            avg_inference_latency, std_inference_latency = np.mean(avg_inference_latency), np.std(avg_inference_latency)\n",
    "            stats[f'avg_inference_latency_{batch_size}'] = avg_inference_latency\n",
    "            stats[f'std_inference_latency_{batch_size}'] = std_inference_latency\n",
    "    elif model_type.startswith('sst'):\n",
    "        print(f'model name: {model_type}')\n",
    "        for batch_size in [64, 1]:\n",
    "            print(f'Running for batch size {batch_size}')\n",
    "            avg_inference_latency = []\n",
    "            for _ in range(num_runs):\n",
    "                val_dl = torch.utils.data.DataLoader(\n",
    "                    dataset=val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False\n",
    "                )\n",
    "                stats['model_size_mb'] = print_size_of_model(model) / 1e6\n",
    "                stats['parameter_count'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                avg_inference_latency.append(trainer.calculate_inference_latency(val_dl))\n",
    "            avg_inference_latency, std_inference_latency = np.mean(avg_inference_latency), np.std(avg_inference_latency)\n",
    "            stats[f'avg_inference_latency_{batch_size}'] = avg_inference_latency\n",
    "            stats[f'std_inference_latency_{batch_size}'] = std_inference_latency\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88c1c7568b3b766b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:36.717821Z",
     "start_time": "2023-10-29T00:42:29.792077Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: mnist\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'mnist', 'acc': 0.9385938593859386, 'model_size_mb': 5.883839, 'parameter_count': 1470474, 'avg_inference_latency_64': 0.0013329408730670905, 'std_inference_latency_64': 0.00015313183800845417, 'avg_inference_latency_1': 0.0001267622632853495, 'std_inference_latency_1': 7.931996021353464e-06}\n"
     ]
    }
   ],
   "source": [
    "mnist_stats = get_stats(5, mnist_model, mnist_trainer, mnist_valid_dataset, 'mnist')\n",
    "print(mnist_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fffc584e37423738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.469833Z",
     "start_time": "2023-10-29T00:42:36.717130Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: sst\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'sst', 'acc': 0.8119266055045872, 'model_size_mb': 5.652849, 'parameter_count': 1412610, 'avg_inference_latency_64': 0.0010027953556605747, 'std_inference_latency_64': 3.19563286115864e-05, 'avg_inference_latency_1': 0.00015513984435195223, 'std_inference_latency_1': 3.554871816429752e-06}\n"
     ]
    }
   ],
   "source": [
    "sst_stats = get_stats(5, sst_model, sst_trainer, sst_dev_dataset, 'sst')\n",
    "print(sst_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c325b37439b236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.472225Z",
     "start_time": "2023-10-29T00:42:38.469294Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([mnist_stats, sst_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f876fd3b4e71335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.474487Z",
     "start_time": "2023-10-29T00:42:38.472426Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b213cbff21c29c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.476723Z",
     "start_time": "2023-10-29T00:42:38.474638Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd599be085ba680f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dynamic Quantization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91597cd02b4cc7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:14:19.937766Z",
     "start_time": "2023-10-29T01:14:19.915182Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization\n",
    "print(torch.backends.quantized.supported_engines)\n",
    "torch.backends.quantized.engine = 'x86'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c6436d1a6a570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T01:14:20.614452Z",
     "start_time": "2023-10-29T01:14:20.542247Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.9386\n",
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.9386\n",
      "model name: mnist_8bit\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'mnist_8bit', 'acc': 0.9385938593859386, 'model_size_mb': 1.480535, 'parameter_count': 0, 'avg_inference_latency_64': 0.0005661390389606452, 'std_inference_latency_64': 6.417420069453933e-05, 'avg_inference_latency_1': 0.00015660790589251797, 'std_inference_latency_1': 3.47960730864735e-06}\n",
      "model name: mnist_16bit\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'mnist_16bit', 'acc': 0.9385938593859386, 'model_size_mb': 5.885655, 'parameter_count': 0, 'avg_inference_latency_64': 0.000959180722570723, 'std_inference_latency_64': 0.00012071004421095663, 'avg_inference_latency_1': 0.0001696667488556717, 'std_inference_latency_1': 1.0326271385712067e-05}\n"
     ]
    }
   ],
   "source": [
    "# MNIST model\n",
    "quantized_mnist_model_8 = torch.quantization.quantize_dynamic(\n",
    "    mnist_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "quantized_mnist_model_16 = torch.quantization.quantize_dynamic(\n",
    "    mnist_model, {torch.nn.Linear}, dtype=torch.float16\n",
    ")\n",
    "\n",
    "mnist_trainer_quantized_8 = Trainer(\n",
    "    model=quantized_mnist_model_8,\n",
    "    train_dl=mnist_dataloader,\n",
    "    valid_dl=mnist_valid_dataloader,\n",
    "    config=mnist_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "mnist_trainer_quantized_8.validate()\n",
    "\n",
    "mnist_trainer_quantized_16 = Trainer(\n",
    "    model=quantized_mnist_model_16,\n",
    "    train_dl=mnist_dataloader,\n",
    "    valid_dl=mnist_valid_dataloader,\n",
    "    config=mnist_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "mnist_trainer_quantized_16.validate()\n",
    "\n",
    "mnist_eight_bit_stats = get_stats(5, quantized_mnist_model_8, mnist_trainer_quantized_8, mnist_valid_dataset, 'mnist_8bit')\n",
    "print(mnist_eight_bit_stats)\n",
    "\n",
    "mnist_sixteen_bit_stats = get_stats(5, quantized_mnist_model_16, mnist_trainer_quantized_16, mnist_valid_dataset, 'mnist_16bit')\n",
    "print(mnist_sixteen_bit_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cef4bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.8096\n",
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.8119\n",
      "model name: sst_8bit\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'sst_8bit', 'acc': 0.8096330275229358, 'model_size_mb': 1.419903, 'parameter_count': 0, 'avg_inference_latency_64': 0.0008473566600254603, 'std_inference_latency_64': 8.789052910645608e-05, 'avg_inference_latency_1': 0.00020241206939067315, 'std_inference_latency_1': 1.8937977372946463e-05}\n",
      "model name: sst_16bit\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'sst_16bit', 'acc': 0.8119266055045872, 'model_size_mb': 5.655295, 'parameter_count': 0, 'avg_inference_latency_64': 0.0010310615812029159, 'std_inference_latency_64': 3.63134646375152e-05, 'avg_inference_latency_1': 0.0002176146988474995, 'std_inference_latency_1': 5.180847951934833e-06}\n"
     ]
    }
   ],
   "source": [
    "# do the same for sst\n",
    "\n",
    "quantized_sst_model_8 = torch.quantization.quantize_dynamic(\n",
    "    sst_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "quantized_sst_model_16 = torch.quantization.quantize_dynamic(\n",
    "    sst_model, {torch.nn.Linear}, dtype=torch.float16\n",
    ")\n",
    "\n",
    "sst_trainer_quantized_8 = Trainer(\n",
    "    model=quantized_sst_model_8,\n",
    "    train_dl=sst_dataloader,\n",
    "    valid_dl=sst_dev_dataloader,\n",
    "    config=sst_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "sst_trainer_quantized_8.validate()\n",
    "\n",
    "sst_trainer_quantized_16 = Trainer(\n",
    "    model=quantized_sst_model_16,\n",
    "    train_dl=sst_dataloader,\n",
    "    valid_dl=sst_dev_dataloader,\n",
    "    config=sst_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "sst_trainer_quantized_16.validate()\n",
    "\n",
    "sst_eight_bit_stats = get_stats(5, quantized_sst_model_8, sst_trainer_quantized_8, sst_dev_dataset, 'sst_8bit')\n",
    "print(sst_eight_bit_stats)\n",
    "\n",
    "sst_sixteen_bit_stats = get_stats(5, quantized_sst_model_16, sst_trainer_quantized_16, sst_dev_dataset, 'sst_16bit')\n",
    "print(sst_sixteen_bit_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36403f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "\n",
    "dynamic_quantization_df = pd.DataFrame([mnist_eight_bit_stats, mnist_sixteen_bit_stats, sst_eight_bit_stats, sst_sixteen_bit_stats])\n",
    "# add baseline stats\n",
    "dynamic_quantization_df = pd.concat([df, dynamic_quantization_df], axis=0)\n",
    "dynamic_quantization_df.to_csv('dynamic_quantization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fd207",
   "metadata": {},
   "source": [
    "# Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a7c2484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent-huang/.virtualenvs/dl-spring-2023/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/vincent-huang/.virtualenvs/dl-spring-2023/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1209: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.4954\n",
      "model name: sst_8bit_static\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'sst_8bit_static', 'acc': 0.4954128440366973, 'model_size_mb': 1.434179, 'parameter_count': 0, 'avg_inference_latency_64': 0.0008141211100987025, 'std_inference_latency_64': 0.00011938487696745565, 'avg_inference_latency_1': 0.00019600331236463076, 'std_inference_latency_1': 2.0684235634766287e-05}\n"
     ]
    }
   ],
   "source": [
    "# static quantization with prepare_fx\n",
    "\n",
    "# do the same for sst\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
    "example_inputs = torch.randn(5001)\n",
    "quantized_sst_model_8 = convert_fx(prepare_fx(sst_model, qconfig_mapping, example_inputs=example_inputs))\n",
    "\n",
    "sst_trainer_quantized_8 = Trainer(\n",
    "    model=quantized_sst_model_8,\n",
    "    train_dl=sst_dataloader,\n",
    "    valid_dl=sst_dev_dataloader,\n",
    "    config=sst_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "sst_trainer_quantized_8.validate()\n",
    "\n",
    "\n",
    "sst_eight_bit_stats = get_stats(5, quantized_sst_model_8, sst_trainer_quantized_8, sst_dev_dataset, 'sst_8bit_static')\n",
    "print(sst_eight_bit_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "159eec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent-huang/.virtualenvs/dl-spring-2023/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/vincent-huang/.virtualenvs/dl-spring-2023/lib/python3.8/site-packages/torch/ao/quantization/observer.py:1209: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.9091\n",
      "model name: mnist_8bit_static\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'mnist_8bit_static', 'acc': 0.9090909090909091, 'model_size_mb': 1.515047, 'parameter_count': 0, 'avg_inference_latency_64': 0.0005392086733678344, 'std_inference_latency_64': 0.0001448632877445155, 'avg_inference_latency_1': 0.0001490873412521306, 'std_inference_latency_1': 4.263607324251732e-06}\n"
     ]
    }
   ],
   "source": [
    "# do the same to mnist\n",
    "\n",
    "quantized_mnist_model_8 = convert_fx(prepare_fx(mnist_model, qconfig_mapping, example_inputs=example_inputs))\n",
    "\n",
    "mnist_trainer_quantized_8 = Trainer(\n",
    "    model=quantized_mnist_model_8,\n",
    "    train_dl=mnist_dataloader,\n",
    "    valid_dl=mnist_valid_dataloader,\n",
    "    config=mnist_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "mnist_trainer_quantized_8.validate()\n",
    "\n",
    "mnist_eight_bit_stats = get_stats(5, quantized_mnist_model_8, mnist_trainer_quantized_8, mnist_valid_dataset, 'mnist_8bit_static')\n",
    "print(mnist_eight_bit_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90430a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "static_quantization_df = pd.DataFrame([mnist_eight_bit_stats, sst_eight_bit_stats])\n",
    "static_quantization_df = pd.concat([dynamic_quantization_df, static_quantization_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bef165b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantization_df.to_csv('static_quantization.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df67d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
