{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:03.364869Z",
     "start_time": "2023-10-29T00:02:03.340161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1089f5d50>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "mnist_config = {\n",
    "    'input_size': 400,\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'hidden_layers': 2,\n",
    "    'hidden_size': 1024,\n",
    "    'epochs': 2,\n",
    "    'log_interval': 10, # log every 10 batches\n",
    "    'output_size': 10\n",
    "}\n",
    "\n",
    "# mnist_config = {\n",
    "#     'input_size': 784,\n",
    "#     'batch_size': 64,\n",
    "#     'lr': 0.001,\n",
    "#     'hidden_layers': 2,\n",
    "#     'hidden_size': 512,\n",
    "#     'epochs': 2,\n",
    "#     'log_interval': 10, # log every 10 batches\n",
    "#     'output_size': 10\n",
    "# }\n",
    "\n",
    "sst_config = {\n",
    "    'batch_size': 64,\n",
    "    'lr': 0.001,\n",
    "    'hidden_layers': 3,\n",
    "    'hidden_size': 256,\n",
    "    'epochs': 2,\n",
    "    'log_interval': 10,\n",
    "    'output_size': 2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:03.590616Z",
     "start_time": "2023-10-29T00:02:03.572686Z"
    }
   },
   "id": "655f662e789486e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SST Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "126bec1910d5c27f"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class SSTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data_type='train', suffix='tsv', vocab=None, top_k=0, remove_stopwords=False):\n",
    "        self.datapath = os.path.join(data_dir, data_type + '.' + suffix)\n",
    "        self.data_type = data_type\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "\n",
    "        with open(self.datapath) as data_f:\n",
    "            for line in data_f:\n",
    "                sentence, label = line.strip().split('\\t')\n",
    "                self.sentences.append(sentence)\n",
    "                self.labels.append(label)\n",
    "        self.sentences = self.sentences[1:]\n",
    "        self.labels = self.labels[1:]\n",
    "        if vocab is None:\n",
    "            self.vocab = self.build_vocab()\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        if top_k > 0:\n",
    "            self.vocab = self.vocab[:top_k]\n",
    "        if remove_stopwords:\n",
    "            self.vocab = self.remove_stopwords()\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # add OOV token\n",
    "        if vocab is None:\n",
    "            self.vocab.append('<OOV>')\n",
    "            self.word2idx['<OOV>'] = len(self.vocab) - 1\n",
    "            self.idx2word[len(self.vocab) - 1] = '<OOV>'\n",
    "            \n",
    "    def remove_stopwords(self):\n",
    "        # remove stopwords obtained from nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        return [word for word in self.vocab if word not in stop_words]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def one_hot_encode(self, word):\n",
    "        vector = torch.zeros(len(self.vocab))\n",
    "        if word in self.word2idx:\n",
    "            vector[self.word2idx[word]] += 1\n",
    "        else:\n",
    "            vector[self.word2idx['<OOV>']] += 1\n",
    "        return vector\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoded_sentence = torch.sum(torch.stack([self.one_hot_encode(word) for word in sentence.split()]), dim=0)\n",
    "        encoded_label = torch.tensor(int(label)) # 0: negative, 1: positive\n",
    "        return encoded_sentence, encoded_label\n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab = {}\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence.split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "        return sorted(vocab, key=vocab.get, reverse=True)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:04.708777Z",
     "start_time": "2023-10-29T00:02:04.698508Z"
    }
   },
   "id": "48a65d0552d834f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8632562d5f4c216c"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data_type='train', suffix='csv', transforms=None):\n",
    "        self.datapath = os.path.join(data_dir, f'mnist_{data_type}.{suffix}')\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(self.datapath) as data_f:\n",
    "            line_cnt = 0\n",
    "            for line in data_f:\n",
    "                if line_cnt > 0:\n",
    "                    data = line.strip().split(',')\n",
    "                    self.labels.append(data[0])\n",
    "                    self.data.append(torch.tensor([float(dp) for dp in data[1:]]))\n",
    "                line_cnt += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx]\n",
    "        # normalize the data\n",
    "        data = (data - torch.mean(data)) / torch.std(data)\n",
    "        if self.transforms is not None:\n",
    "            # reconstruct to image and do transforms\n",
    "            data = data.reshape(1, 28, 28)\n",
    "            data = self.transforms(data)\n",
    "            # flatten\n",
    "            data = data.reshape(-1)\n",
    "        return data, torch.tensor(int(label))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:05.834736Z",
     "start_time": "2023-10-29T00:02:05.815413Z"
    }
   },
   "id": "a71ca22088b15399"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bbc8e3714b50a46"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.total_flops = 0\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_idx in range(self.num_hidden_layers):\n",
    "            if layer_idx == 0:\n",
    "                self.layers.append(torch.nn.Linear(self.input_size, self.hidden_size))\n",
    "                self.total_flops += self.input_size * self.hidden_size\n",
    "            else:\n",
    "                self.layers.append(torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "                self.total_flops += self.hidden_size * self.hidden_size\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.total_flops += self.hidden_size\n",
    "        self.layers.append(torch.nn.Linear(self.hidden_size, self.output_size))\n",
    "        self.total_flops += self.hidden_size * self.output_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.total_flops += self.output_size * np.log2(self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_size)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.softmax(x) \n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:02:06.936730Z",
     "start_time": "2023-10-29T00:02:06.925034Z"
    }
   },
   "id": "d4b112e66d849660"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28fe6efdb3772bc4"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 train_dl, \n",
    "                 valid_dl, \n",
    "                 config, \n",
    "                 ckpt_path='ckpt', \n",
    "                 device='cuda', \n",
    "                 save_ckpts=False,\n",
    "                 no_train=False):\n",
    "        self.model = model.to(device)\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.config = config\n",
    "        self.no_train = no_train\n",
    "        if not no_train:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['lr'])\n",
    "            self.criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.training_time = 0\n",
    "        self.training_time_per_epoch = []\n",
    "        self.total_inference_time = 0\n",
    "        self.inference_time = []\n",
    "        self.avg_inference_time_per_epoch = []\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.best_acc = 0\n",
    "        self.device = device\n",
    "        self.save_ckpt = save_ckpts\n",
    "\n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            start_time = time.time()\n",
    "            self.train_one_epoch()\n",
    "            end_time = time.time()\n",
    "            self.training_time_per_epoch.append(end_time - start_time)\n",
    "            acc = self.validate()\n",
    "            if acc > self.best_acc:\n",
    "                self.best_acc = acc\n",
    "                if self.save_ckpt:\n",
    "                    self.save(epoch)\n",
    "        end = time.time()\n",
    "        self.training_time = end - start\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        batch_bar = tqdm.tqdm(total=len(self.train_dl), dynamic_ncols=True, leave=True, position=0, desc='Train',\n",
    "                              ncols=5)\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "        total_inference_epoch = 0\n",
    "\n",
    "        for idx, (train_data, train_label) in enumerate(self.train_dl):\n",
    "            unit_num_correct, unit_total_loss, batch_inference_time = self.train_one_batch(train_data, train_label)\n",
    "            total_inference_epoch += batch_inference_time\n",
    "            num_correct += unit_num_correct\n",
    "            total_loss += unit_total_loss\n",
    "            batch_bar.set_postfix(\n",
    "                acc=f\"{100 * num_correct / (self.config['batch_size'] * (idx + 1)):.4f}\",\n",
    "                loss=f\"{total_loss / (self.config['batch_size'] * (idx + 1)):.4f}\",\n",
    "                num_correct=f\"{num_correct}\",\n",
    "                lr=f\"{self.optimizer.param_groups[0]['lr']:.4f}\"\n",
    "            )\n",
    "            batch_bar.update()\n",
    "\n",
    "        self.avg_inference_time_per_epoch.append(total_inference_epoch / len(self.train_dl.dataset))\n",
    "        batch_bar.close()\n",
    "\n",
    "    def train_one_batch(self, train_data, train_label):\n",
    "        train_data = train_data.to(self.device)\n",
    "        train_label = train_label.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        inference_start = time.time()\n",
    "        output = self.model(train_data)\n",
    "        inference_end = time.time()\n",
    "        self.total_inference_time += inference_end - inference_start\n",
    "        # l2 regularization\n",
    "        # loss = self.criterion(output, train_label) - 0.001 * torch.norm(self.model.layers[0].weight, p=2)\n",
    "        loss = self.criterion(output, train_label)\n",
    "        num_correct = torch.sum(torch.argmax(output, dim=1) == train_label).item()\n",
    "        total_loss = float(loss.item())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return num_correct, total_loss, inference_end - inference_start\n",
    "\n",
    "    def validate(self):\n",
    "        # report accuracy\n",
    "        correct_count = 0\n",
    "        valid_loss = 0\n",
    "        num_samples = 0\n",
    "        for idx, (valid_data, valid_label) in enumerate(self.valid_dl):\n",
    "            num_samples += len(valid_data)\n",
    "            valid_data = valid_data.to(self.device)\n",
    "            valid_label = valid_label.to(self.device)\n",
    "            output = self.model(valid_data)\n",
    "            pred = torch.argmax(output, dim=1)  # (batch_size)\n",
    "            correct_count += torch.sum(pred == valid_label).item()\n",
    "            if not self.no_train:\n",
    "                loss = self.criterion(output, valid_label).item()\n",
    "                valid_loss += loss\n",
    "        if not self.no_train:\n",
    "            valid_loss /= num_samples\n",
    "        acc = correct_count / len(self.valid_dl.dataset)\n",
    "        print(f'\\nValidation Loss: {valid_loss if not self.no_train else \"n/a\"}\\t Validation Accuracy: {correct_count / len(self.valid_dl.dataset)}')\n",
    "        return acc\n",
    "    \n",
    "    def calculate_inference_latency(self, val_dl):\n",
    "        # calculate inference latency per batch\n",
    "        total_inference_time = 0\n",
    "        num_of_batches = 0\n",
    "        for idx, (valid_data, valid_label) in enumerate(val_dl):\n",
    "            num_of_batches += 1\n",
    "            valid_data = valid_data.to(self.device)\n",
    "            inference_start = time.time()\n",
    "            _ = self.model(valid_data)\n",
    "            inference_end = time.time()\n",
    "            total_inference_time += inference_end - inference_start\n",
    "        return total_inference_time / num_of_batches\n",
    "\n",
    "    def save(self, epoch):\n",
    "        ckpt_dir = os.path.join(self.ckpt_path, f'{time.strftime(\"%m-%d-%H-%M\", time.localtime())}')\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        print(f'Saving model at Epoch {epoch}')\n",
    "        ckpt_path = os.path.join(ckpt_dir, f'best_acc.pt')\n",
    "        torch.save(self.model.state_dict(), ckpt_path)\n",
    "\n",
    "    def get_training_stats(self):\n",
    "        # we need flops, avg inference per sample and accuracy\n",
    "        return {\n",
    "            'num_parameters': self.model.param_count,\n",
    "            'num_flops': self.model.flops,\n",
    "            'training_time': self.training_time,\n",
    "            'training_time_per_epoch': np.mean(self.training_time_per_epoch),\n",
    "            'best_acc': self.best_acc,\n",
    "            'average_inference_time_per_sample': self.total_inference_time / (self.config['epochs'] * len(self.train_dl.dataset)),\n",
    "        }\n",
    "    def report(self):\n",
    "        print(f'Number of parameters: {self.model.count_parameters()}')\n",
    "        print(f'Total number of FLOPs: {self.model.total_flops}')\n",
    "        print(f'Average training time per epoch: {np.mean(self.training_time_per_epoch)} seconds')\n",
    "        print(f'Training time: {self.training_time} seconds')\n",
    "        print(\n",
    "            f'Average inference time per sample: {self.total_inference_time / (self.config[\"epochs\"] * len(self.train_dl.dataset))} seconds')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:00:11.045603Z",
     "start_time": "2023-10-29T01:00:11.039532Z"
    }
   },
   "id": "7fb9c4ad3dc03d9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e427d5891938274"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "from torchvision import transforms\n",
    "\n",
    "sst_train_dataset = SSTDataset(data_dir='SST-2', data_type='train', top_k=5000)\n",
    "sst_dev_dataset = SSTDataset(data_dir='SST-2', data_type='dev', vocab=sst_train_dataset.vocab)\n",
    "mnist_train_dataset = MNISTDataset(data_dir='MNIST', data_type='train', transforms=transforms.CenterCrop((20, 20)))\n",
    "mnist_valid_dataset = MNISTDataset(data_dir='MNIST', data_type='test', transforms=transforms.CenterCrop((20, 20)))\n",
    "\n",
    "\n",
    "sst_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=sst_train_dataset,\n",
    "    batch_size=sst_config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "sst_dev_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=sst_dev_dataset,\n",
    "    batch_size=sst_config['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "mnist_dataloader = torch.utils.data.DataLoader(dataset=mnist_train_dataset,\n",
    "                                               batch_size=mnist_config['batch_size'],\n",
    "                                               shuffle=True)\n",
    "mnist_valid_dataloader = torch.utils.data.DataLoader(dataset=mnist_valid_dataset,\n",
    "                                                     batch_size=mnist_config['batch_size'],\n",
    "                                                     shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:03:58.899397Z",
     "start_time": "2023-10-29T00:03:50.310844Z"
    }
   },
   "id": "d38dacbac82ea52"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:03:59.853293Z",
     "start_time": "2023-10-29T00:03:59.835002Z"
    }
   },
   "id": "e46234eb11c0a5e9"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:06<00:00, 138.93it/s, acc=90.5967, loss=0.0243, lr=0.0010, num_correct=54387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.023860583079792354\t Validation Accuracy: 0.9411941194119412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 938/938 [00:06<00:00, 138.61it/s, acc=93.7450, loss=0.0238, lr=0.0010, num_correct=56277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.023877536336092107\t Validation Accuracy: 0.93999399939994\n",
      "Number of parameters: 1470474\n",
      "Total number of FLOPs: 1470497.2192809489\n",
      "Average training time per epoch: 6.760261416435242 seconds\n",
      "Training time: 14.094148874282837 seconds\n",
      "Average inference time per sample: 1.3637519677095937e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "# MNIST model\n",
    "mnist_model = MLP(input_size=mnist_config['input_size'], hidden_size=mnist_config['hidden_size'], num_hidden_layers=mnist_config['hidden_layers'], output_size=mnist_config['output_size'])\n",
    "\n",
    "mnist_trainer = Trainer(\n",
    "    model=mnist_model,\n",
    "    train_dl=mnist_dataloader,\n",
    "    valid_dl=mnist_valid_dataloader,\n",
    "    config=mnist_config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "mnist_trainer.train()\n",
    "mnist_trainer.report()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:04:14.425035Z",
     "start_time": "2023-10-29T00:04:00.324277Z"
    }
   },
   "id": "ce7a477a541fd203"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1053/1053 [00:10<00:00, 99.11it/s, acc=82.3392, loss=0.0075, lr=0.0010, num_correct=55490] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.008014699495440229\t Validation Accuracy: 0.8107798165137615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 1053/1053 [00:10<00:00, 99.00it/s, acc=89.5878, loss=0.0064, lr=0.0010, num_correct=60375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: 0.007898959681528425\t Validation Accuracy: 0.819954128440367\n"
     ]
    }
   ],
   "source": [
    "# SST model\n",
    "sst_model = MLP(input_size=len(sst_train_dataset.vocab), \n",
    "                hidden_size=sst_config['hidden_size'], \n",
    "                num_hidden_layers=sst_config['hidden_layers'], \n",
    "                output_size=sst_config['output_size'])\n",
    "\n",
    "sst_trainer = Trainer(\n",
    "    model=sst_model,\n",
    "    train_dl=sst_dataloader,\n",
    "    valid_dl=sst_dev_dataloader,\n",
    "    config=sst_config,\n",
    "    device=device\n",
    ")\n",
    "sst_trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:06:31.509950Z",
     "start_time": "2023-10-29T00:06:09.970171Z"
    }
   },
   "id": "134b0ce4c68f679c"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# get stats for baseline, \n",
    "#   - inference latency for batch size 1 and 64 over 5 runs\n",
    "#   - model size\n",
    "#   - parameter counts\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    # print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "def get_stats(num_runs,\n",
    "              model,\n",
    "              trainer,\n",
    "              val_ds,\n",
    "              model_type):\n",
    "    stats = {}\n",
    "    stats['model_name'] = model_type\n",
    "\n",
    "    stats['acc'] = trainer.best_acc\n",
    "    if model_type.startswith('mnist'):\n",
    "        print(f'model name: {model_type}')\n",
    "        for batch_size in [64, 1]:\n",
    "            print(f'Running for batch size {batch_size}')\n",
    "            avg_inference_latency = []\n",
    "            for _ in range(num_runs):\n",
    "                val_dl = torch.utils.data.DataLoader(\n",
    "                    dataset=val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False\n",
    "                )\n",
    "                stats['model_size_mb'] = print_size_of_model(model) / 1e6\n",
    "                stats['parameter_count'] = model.count_parameters()\n",
    "                avg_inference_latency.append(trainer.calculate_inference_latency(val_dl))\n",
    "            avg_inference_latency, std_inference_latency = np.mean(avg_inference_latency), np.std(avg_inference_latency)\n",
    "            stats[f'avg_inference_latency_{batch_size}'] = avg_inference_latency\n",
    "            stats[f'std_inference_latency_{batch_size}'] = std_inference_latency\n",
    "    elif model_type.startswith('sst'):\n",
    "        print(f'model name: {model_type}')\n",
    "        for batch_size in [64, 1]:\n",
    "            print(f'Running for batch size {batch_size}')\n",
    "            avg_inference_latency = []\n",
    "            for _ in range(num_runs):\n",
    "                val_dl = torch.utils.data.DataLoader(\n",
    "                    dataset=val_ds,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False\n",
    "                )\n",
    "                stats['model_size'] = print_size_of_model(model) / 1e6\n",
    "                stats['parameter_count'] = model.count_parameters()\n",
    "                avg_inference_latency.append(trainer.calculate_inference_latency(val_dl))\n",
    "            avg_inference_latency, std_inference_latency = np.mean(avg_inference_latency), np.std(avg_inference_latency)\n",
    "            stats[f'avg_inference_latency_{batch_size}'] = avg_inference_latency\n",
    "            stats[f'std_inference_latency_{batch_size}'] = std_inference_latency\n",
    "        \n",
    "    return stats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:28.890678Z",
     "start_time": "2023-10-29T00:42:28.876450Z"
    }
   },
   "id": "dd005a9a2306f8b2"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: mnist\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'mnist', 'acc': 0.9411941194119412, 'model_size_mb': 5.883839, 'parameter_count': 1470474, 'avg_inference_latency_64': 0.0005422801728461199, 'std_inference_latency_64': 0.00011336873448196703, 'avg_inference_latency_1': 6.376058653076002e-05, 'std_inference_latency_1': 5.088073975139941e-06}\n"
     ]
    }
   ],
   "source": [
    "mnist_stats = get_stats(5, mnist_model, mnist_trainer, mnist_valid_dataset, 'mnist')\n",
    "print(mnist_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:36.717821Z",
     "start_time": "2023-10-29T00:42:29.792077Z"
    }
   },
   "id": "88c1c7568b3b766b"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: sst\n",
      "Running for batch size 64\n",
      "Running for batch size 1\n",
      "{'model_name': 'sst', 'acc': 0.819954128440367, 'model_size': 5.652849, 'parameter_count': 1412610, 'avg_inference_latency_64': 0.0006016629082815988, 'std_inference_latency_64': 2.821245048805144e-05, 'avg_inference_latency_1': 7.584997273366385e-05, 'std_inference_latency_1': 4.373535918128275e-07}\n"
     ]
    }
   ],
   "source": [
    "sst_stats = get_stats(5, sst_model, sst_trainer, sst_dev_dataset, 'sst')\n",
    "print(sst_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.469833Z",
     "start_time": "2023-10-29T00:42:36.717130Z"
    }
   },
   "id": "fffc584e37423738"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([mnist_stats, sst_stats])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.472225Z",
     "start_time": "2023-10-29T00:42:38.469294Z"
    }
   },
   "id": "6c325b37439b236"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "df.to_csv('baseline.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.474487Z",
     "start_time": "2023-10-29T00:42:38.472426Z"
    }
   },
   "id": "f876fd3b4e71335"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:42:38.476723Z",
     "start_time": "2023-10-29T00:42:38.474638Z"
    }
   },
   "id": "8b213cbff21c29c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dynamic Quantization with PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd599be085ba680f"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qnnpack', 'none']\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization\n",
    "print(torch.backends.quantized.supported_engines)\n",
    "torch.backends.quantized.engine = 'qnnpack'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:14:19.937766Z",
     "start_time": "2023-10-29T01:14:19.915182Z"
    }
   },
   "id": "e91597cd02b4cc7b"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "quantized::linear_prepack_fp16 is currently not supported by QNNPACK",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[124], line 8\u001B[0m\n\u001B[1;32m      2\u001B[0m mnist_model \u001B[38;5;241m=\u001B[39m MLP(input_size\u001B[38;5;241m=\u001B[39mmnist_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_size\u001B[39m\u001B[38;5;124m'\u001B[39m], hidden_size\u001B[38;5;241m=\u001B[39mmnist_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_size\u001B[39m\u001B[38;5;124m'\u001B[39m], num_hidden_layers\u001B[38;5;241m=\u001B[39mmnist_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhidden_layers\u001B[39m\u001B[38;5;124m'\u001B[39m], output_size\u001B[38;5;241m=\u001B[39mmnist_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput_size\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m quantized_mnist_model_8 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mquantization\u001B[38;5;241m.\u001B[39mquantize_dynamic(\n\u001B[1;32m      5\u001B[0m     mnist_model, {torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mLinear}, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mqint8\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m quantized_mnist_model_16 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantize_dynamic\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmnist_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:466\u001B[0m, in \u001B[0;36mquantize_dynamic\u001B[0;34m(model, qconfig_spec, dtype, mapping, inplace)\u001B[0m\n\u001B[1;32m    464\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m    465\u001B[0m propagate_qconfig_(model, qconfig_spec)\n\u001B[0;32m--> 466\u001B[0m \u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:551\u001B[0m, in \u001B[0;36mconvert\u001B[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inplace:\n\u001B[1;32m    550\u001B[0m     module \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(module)\n\u001B[0;32m--> 551\u001B[0m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remove_qconfig:\n\u001B[1;32m    555\u001B[0m     _remove_qconfig(module)\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:589\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    584\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, mod \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;66;03m# both fused modules and observed custom modules are\u001B[39;00m\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;66;03m# swapped as one unit\u001B[39;00m\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    588\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[0;32m--> 589\u001B[0m         \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# inplace\u001B[39;49;00m\n\u001B[1;32m    590\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mis_reference\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_custom_config_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:591\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001B[0m\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mod, _FusedModule) \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    588\u001B[0m        type_before_parametrizations(mod) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m custom_module_class_mapping:\n\u001B[1;32m    589\u001B[0m         _convert(mod, mapping, \u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# inplace\u001B[39;00m\n\u001B[1;32m    590\u001B[0m                  is_reference, convert_custom_config_dict)\n\u001B[0;32m--> 591\u001B[0m     reassign[name] \u001B[38;5;241m=\u001B[39m \u001B[43mswap_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_module_class_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m reassign\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    594\u001B[0m     module\u001B[38;5;241m.\u001B[39m_modules[key] \u001B[38;5;241m=\u001B[39m value\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/quantization/quantize.py:624\u001B[0m, in \u001B[0;36mswap_module\u001B[0;34m(mod, mapping, custom_module_class_mapping)\u001B[0m\n\u001B[1;32m    622\u001B[0m         new_mod \u001B[38;5;241m=\u001B[39m qmod\u001B[38;5;241m.\u001B[39mfrom_float(mod, weight_qparams)\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 624\u001B[0m         new_mod \u001B[38;5;241m=\u001B[39m \u001B[43mqmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_float\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    625\u001B[0m     swapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m swapped:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001B[39;00m\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.from_float\u001B[0;34m(cls, mod)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnsupported dtype specified for dynamic quantized Linear!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 116\u001B[0m qlinear \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m qlinear\u001B[38;5;241m.\u001B[39mset_weight_bias(qweight, mod\u001B[38;5;241m.\u001B[39mbias)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m qlinear\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py:40\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, in_features, out_features, bias_, dtype)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, in_features, out_features, bias_\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mqint8):\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43min_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# We don't muck around with buffers or attributes or anything here\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;66;03m# to keep the module simple. *everything* is simply a Python attribute.\u001B[39;00m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;66;03m# Serialization logic is explicitly handled in the below serialization and\u001B[39;00m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;66;03m# deserialization modules\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mversion \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/linear.py:151\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, in_features, out_features, bias_, dtype)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnsupported dtype specified for quantized Linear!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 151\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m \u001B[43mLinearPackedParams\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params\u001B[38;5;241m.\u001B[39mset_weight_bias(qweight, bias)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/linear.py:27\u001B[0m, in \u001B[0;36mLinearPackedParams.__init__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat16:\n\u001B[1;32m     26\u001B[0m     wq \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros([\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_weight_bias\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/linear.py:34\u001B[0m, in \u001B[0;36mLinearPackedParams.set_weight_bias\u001B[0;34m(self, weight, bias)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mquantized\u001B[38;5;241m.\u001B[39mlinear_prepack(weight, bias)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat16:\n\u001B[0;32m---> 34\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquantized\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear_prepack_fp16\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnsupported dtype on dynamic quantized linear!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/_ops.py:502\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    498\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    500\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 502\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: quantized::linear_prepack_fp16 is currently not supported by QNNPACK"
     ]
    }
   ],
   "source": [
    "# MNIST model\n",
    "mnist_model = MLP(input_size=mnist_config['input_size'], hidden_size=mnist_config['hidden_size'], num_hidden_layers=mnist_config['hidden_layers'], output_size=mnist_config['output_size'])\n",
    "\n",
    "quantized_mnist_model_8 = torch.quantization.quantize_dynamic(\n",
    "    mnist_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "quantized_mnist_model_16 = torch.quantization.quantize_dynamic(\n",
    "    mnist_model, {torch.nn.Linear}, dtype=torch.float16\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:14:20.614452Z",
     "start_time": "2023-10-29T01:14:20.542247Z"
    }
   },
   "id": "80c6436d1a6a570"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc02cd230a8e64a8"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Loss: n/a\t Validation Accuracy: 0.1338133813381338\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.1338133813381338"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trainer_quantized_8 = Trainer(\n",
    "    model=quantized_mnist_model_8,\n",
    "    train_dl=mnist_dataloader,\n",
    "    valid_dl=mnist_valid_dataloader,\n",
    "    config=mnist_config,\n",
    "    device=device,\n",
    "    no_train=True\n",
    ")\n",
    "mnist_trainer_quantized_8.validate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T01:00:22.646867Z",
     "start_time": "2023-10-29T01:00:22.278041Z"
    }
   },
   "id": "faf857e4476b531"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[110], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m mnist_trainer_8bit \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmnist_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_valid_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquantized_mnist_model_8\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m eight_bit_stats \u001B[38;5;241m=\u001B[39m get_stats(\u001B[38;5;241m5\u001B[39m, quantized_mnist_model_8, mnist_trainer_8bit, mnist_valid_dataset, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmnist_8bit\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(eight_bit_stats)\n",
      "Cell \u001B[0;32mIn[108], line 2\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(train_dl, val_dl, model, config, device, save_ckpt)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model\u001B[39m(train_dl, val_dl, model, config, device, save_ckpt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m----> 2\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalid_dl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_ckpts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_ckpt\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer\n",
      "Cell \u001B[0;32mIn[51], line 17\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, model, train_dl, valid_dl, config, ckpt_path, device, save_ckpts)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdam\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_time_per_epoch \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/optim/adam.py:33\u001B[0m, in \u001B[0;36mAdam.__init__\u001B[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid weight_decay value: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(weight_decay))\n\u001B[1;32m     29\u001B[0m defaults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(lr\u001B[38;5;241m=\u001B[39mlr, betas\u001B[38;5;241m=\u001B[39mbetas, eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m     30\u001B[0m                 weight_decay\u001B[38;5;241m=\u001B[39mweight_decay, amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m     31\u001B[0m                 maximize\u001B[38;5;241m=\u001B[39mmaximize, foreach\u001B[38;5;241m=\u001B[39mforeach, capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m     32\u001B[0m                 differentiable\u001B[38;5;241m=\u001B[39mdifferentiable, fused\u001B[38;5;241m=\u001B[39mfused)\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefaults\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fused:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m differentiable:\n",
      "File \u001B[0;32m~/Development/on-device-ml/on-device-ml/lib/python3.9/site-packages/torch/optim/optimizer.py:187\u001B[0m, in \u001B[0;36mOptimizer.__init__\u001B[0;34m(self, params, defaults)\u001B[0m\n\u001B[1;32m    185\u001B[0m param_groups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(params)\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(param_groups) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer got an empty parameter list\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(param_groups[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    189\u001B[0m     param_groups \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: param_groups}]\n",
      "\u001B[0;31mValueError\u001B[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "eight_bit_stats = get_stats(5, quantized_mnist_model_8, mnist_trainer, mnist_valid_dataset, 'mnist_8bit')\n",
    "print(eight_bit_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T00:48:30.357641Z",
     "start_time": "2023-10-29T00:48:23.312631Z"
    }
   },
   "id": "a20c349ba6bb17b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da5f555ebe71dcef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
